# ðŸŽ¯ AI Ranking Functionality - Improvement Suggestions

## Current Implementation Analysis

Your AI reranking system already has:
- âœ… Semantic relevance scoring (40%)
- âœ… User intent matching (30%)
- âœ… Content quality assessment (20%)
- âœ… Specificity scoring (10%)
- âœ… Custom instructions support
- âœ… Post type priority handling
- âœ… Hybrid score combining TF-IDF + AI scores

## ðŸš€ Improvement Suggestions

### 1. **Enhance Context in Prompt** â­ CRITICAL PRIORITY

**Problem:** LLM only sees title, excerpt, type, and TF-IDF score. Missing key signals.

**Current Format:**
```
Result 1 (ID: 123):
Title: ...
Type: ...
Excerpt: ...
TF-IDF Score: 0.85
```

**Enhanced Format:**
```python
def _format_results_for_reranking(self, results: List[Dict[str, Any]]) -> str:
    """Enhanced formatting with more context signals."""
    formatted = []
    for i, result in enumerate(results, 1):
        excerpt = result.get('excerpt', '')
        if len(excerpt) > 300:
            excerpt = excerpt[:300] + '...'
        
        # Extract additional signals
        date = result.get('date', '')
        word_count = result.get('word_count', 0)
        categories = result.get('categories', [])
        tags = result.get('tags', [])
        featured_image = result.get('featured_image', '')
        
        # Calculate freshness
        freshness_indicator = self._get_freshness_indicator(date)
        
        formatted.append(f"""
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Result {i} (ID: {result['id']}):
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ðŸ“Œ Title: {result['title']}
ðŸ·ï¸  Type: {result.get('type', 'unknown')}
ðŸ“… Published: {date} {freshness_indicator}
ðŸ“ Excerpt: {excerpt}
ðŸ“Š Word Count: {word_count} words
ðŸ·ï¸  Categories: {', '.join([cat.get('name', '') for cat in categories[:3]])}
ðŸ·ï¸  Tags: {', '.join([tag.get('name', '') for tag in tags[:5]])}
ðŸ–¼ï¸  Has Image: {'Yes' if featured_image else 'No'}
â­ TF-IDF Score: {result.get('score', 0):.3f}
ðŸ”— URL: {result.get('url', '')}
""")
    return "\n".join(formatted)

def _get_freshness_indicator(self, date: str) -> str:
    """Get freshness indicator for date."""
    if not date:
        return ""
    try:
        from datetime import datetime
        post_dt = datetime.strptime(date, '%Y-%m-%d')
        days_old = (datetime.now() - post_dt).days
        
        if days_old < 30:
            return "(ðŸ†• Very Recent)"
        elif days_old < 90:
            return "(âœ¨ Recent)"
        elif days_old < 180:
            return "(ðŸ“… Recent-ish)"
        else:
            return "(â° Older)"
    except:
        return ""
```

**Impact:** LLM has more context to make better relevance judgments.

---

### 2. **Improved Scoring Criteria** â­ HIGH PRIORITY

**Current:** Fixed weights (40/30/20/10)

**Enhanced:** Context-aware scoring with better examples

```python
# Enhanced scoring prompt
scoring_criteria = f"""
ðŸ“Š SCORING CRITERIA (Rate each result 0-100):

1. **Semantic Relevance** (40 points)
   - Does the content match the query's semantic meaning?
   - Is it exactly what the user is looking for?
   - For person names: Exact name match = 95+, partial = 70-90
   - For services: Service page = 90+, case study = 75-85, blog = 60-70
   - For technical terms: Exact match = 90+, related = 70-85
   
   EXAMPLES:
   âœ… Query: "James Walsh" â†’ Result: "James Walsh, CEO" (Score: 98 - exact match)
   âœ… Query: "hazardous waste disposal" â†’ Result: "Hazardous Waste Management Services" (Score: 92 - exact service)
   âœ… Query: "environmental compliance" â†’ Result: "Environmental Compliance Consulting" (Score: 88 - exact match)
   âœ… Query: "PFAS remediation" â†’ Result: "PFAS Treatment Solutions" (Score: 85 - conceptually related)
   âŒ Query: "water treatment" â†’ Result: "Solid Waste Management" (Score: 25 - not relevant)

2. **User Intent Matching** (30 points)
   - Does it address what the user wants to accomplish?
   - Consider detected query intent when scoring
   
   INTENT-SPECIFIC SCORING:
   â€¢ PERSON NAME QUERIES:
     - scs-professionals profile with exact name â†’ 95-100
     - Article prominently featuring person â†’ 80-90
     - Article mentioning person in passing â†’ 60-70
     - Generic content â†’ 20-40
   
   â€¢ SERVICE QUERIES:
     - scs-services page (service landing page) â†’ 95-100
     - Service-focused case study â†’ 85-95
     - Blog post about service â†’ 65-80
     - Generic mention â†’ 40-60
   
   â€¢ HOW-TO / INFORMATIONAL QUERIES:
     - Step-by-step guide/article â†’ 90-100
     - Comprehensive case study â†’ 75-85
     - Brief mention â†’ 50-65
     - General page â†’ 40-60
   
   â€¢ NAVIGATIONAL QUERIES ("contact", "about"):
     - Exact page match â†’ 100
     - Related page â†’ 70-85
     - Article â†’ 30-50
   
   â€¢ TRANSACTIONAL QUERIES:
     - Contact/request page â†’ 95-100
     - Service description â†’ 75-85
     - Blog post â†’ 50-65

3. **Content Quality & Completeness** (20 points)
   - Based on title, excerpt, word count, and metadata
   - Comprehensive content gets higher scores
   
   QUALITY FACTORS:
   â€¢ Word Count: 2000+ words = 18-20 pts, 1000-2000 = 15-18 pts, 500-1000 = 12-15 pts, <500 = 8-12 pts
   â€¢ Has Excerpt: +2 points (well-written content)
   â€¢ Has Featured Image: +1 point (visual content)
   â€¢ Categories Match Query: +2 points per matching category
   â€¢ Tags Match Query: +1 point per matching tag
   â€¢ Recent Content (< 6 months): +1-2 points (freshness bonus)

4. **Specificity & Depth** (10 points)
   - Is it specifically about the topic or too broad/general?
   - Does it cover the exact aspect the user asked about?
   
   SPECIFICITY SCORING:
   â€¢ Exact topic match (query appears in title/excerpt) â†’ 8-10 pts
   â€¢ Specific subtopic â†’ 6-8 pts
   â€¢ Related general topic â†’ 4-6 pts
   â€¢ Too broad/general â†’ 1-3 pts

5. **Freshness Bonus** (up to +5 bonus points)
   - Very recent content (< 30 days): +5 points
   - Recent content (30-90 days): +3 points
   - Recent-ish (90-180 days): +1 point
   - Older: +0 points

{f"6. **Custom Criteria** (HIGHEST PRIORITY - can override above):{chr(10)}{custom_instructions}" if custom_instructions else ""}

âš ï¸ IMPORTANT SCORING RULES:
- Be strict but fair - differentiate between exact matches and related content
- For person names: Prioritize professional profiles over articles
- For services: Prioritize service pages over blog posts
- Quality matters: Comprehensive content > brief mentions
- Freshness matters: Recent content > old content (within same relevance)
- Use full 0-100 scale: Don't cluster scores around 70-80
"""
```

**Impact:** More nuanced scoring that better reflects relevance.

---

### 3. **Smart Score Normalization** â­ HIGH PRIORITY

**Problem:** AI scores might be inconsistent (some queries get 70-90, others get 50-70)

**Solution:** Normalize AI scores relative to the result set

```python
def _normalize_ai_scores(self, ai_scores: List[Dict[str, Any]], results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    Normalize AI scores to ensure good distribution.
    This ensures scores use the full 0-100 range effectively.
    """
    if not ai_scores:
        return ai_scores
    
    # Extract scores
    scores = [r['ai_score'] for r in ai_scores]
    
    # Calculate min/max
    min_score = min(scores)
    max_score = max(scores)
    score_range = max_score - min_score
    
    # If all scores are similar, use percentile-based normalization
    if score_range < 20:  # Scores are clustered
        logger.info(f"AI scores clustered (range: {score_range}), applying percentile normalization")
        
        # Sort scores
        sorted_scores = sorted(scores)
        n = len(sorted_scores)
        
        # Map to percentile-based scale
        normalized = []
        for result in ai_scores:
            score = result['ai_score']
            # Find percentile
            percentile = sorted_scores.index(score) / n
            
            # Map to 0-100 scale with better distribution
            if percentile < 0.1:
                normalized_score = 60 + (percentile / 0.1) * 15  # 60-75
            elif percentile < 0.3:
                normalized_score = 75 + ((percentile - 0.1) / 0.2) * 10  # 75-85
            elif percentile < 0.7:
                normalized_score = 85 + ((percentile - 0.3) / 0.4) * 10  # 85-95
            else:
                normalized_score = 95 + ((percentile - 0.7) / 0.3) * 5  # 95-100
            
            result['ai_score'] = int(normalized_score)
            result['ai_score_normalized'] = True
        
        return ai_scores
    
    # If scores are well-distributed, just ensure they're in 0-100 range
    if min_score < 0 or max_score > 100:
        logger.info(f"Normalizing scores from [{min_score}, {max_score}] to [0, 100]")
        for result in ai_scores:
            if score_range > 0:
                normalized = ((result['ai_score'] - min_score) / score_range) * 100
                result['ai_score'] = int(normalized)
            else:
                result['ai_score'] = 75  # Default if all same
    
    return ai_scores
```

**Impact:** More consistent scoring across different queries.

---

### 4. **Dynamic Hybrid Score Weighting** â­ MEDIUM PRIORITY

**Problem:** Fixed 70/30 AI/TF-IDF ratio doesn't adapt to query type

**Solution:** Adjust weights based on query characteristics

```python
def _calculate_dynamic_weights(self, query: str, query_intent: str, ai_weight: float) -> tuple[float, float]:
    """
    Calculate dynamic weights based on query characteristics.
    
    Returns:
        (ai_weight, tfidf_weight) tuple
    """
    base_ai_weight = ai_weight
    base_tfidf_weight = 1.0 - ai_weight
    
    # For exact phrase queries, boost TF-IDF (keyword matching matters more)
    if '"' in query or len(query.split()) <= 2:
        # Exact phrase or short query
        ai_weight = base_ai_weight * 0.8  # Reduce AI weight
        tfidf_weight = 1.0 - ai_weight
        logger.debug(f"Short/exact query detected, reducing AI weight to {ai_weight:.2f}")
    
    # For semantic queries (longer, conceptual), boost AI
    elif len(query.split()) > 5 or query_intent in ['howto', 'informational']:
        ai_weight = min(base_ai_weight * 1.1, 0.85)  # Increase AI weight
        tfidf_weight = 1.0 - ai_weight
        logger.debug(f"Semantic query detected, increasing AI weight to {ai_weight:.2f}")
    
    # For person names, boost AI (semantic understanding matters more)
    elif query_intent == 'person_name':
        ai_weight = min(base_ai_weight * 1.15, 0.9)  # Strong AI boost
        tfidf_weight = 1.0 - ai_weight
        logger.debug(f"Person name query, boosting AI weight to {ai_weight:.2f}")
    
    else:
        ai_weight = base_ai_weight
        tfidf_weight = base_tfidf_weight
    
    return ai_weight, tfidf_weight
```

**Impact:** Better results for different query types.

---

### 5. **Better Error Handling & Fallback** â­ MEDIUM PRIORITY

**Problem:** If AI fails or returns incomplete scores, some results get no score

**Solution:** Smart fallback scoring

```python
# In rerank_results_async, after parsing AI scores:

# Enhanced fallback for missing scores
reranked_results = []
ai_score_map = {str(r.get('id')): r for r in ai_scores}

for result in results:
    result_id = str(result['id'])
    ai_result = ai_score_map.get(result_id)
    
    if ai_result:
        tfidf_score = result.get('score', 0.0)
        ai_score = ai_result['ai_score'] / 100  # Normalize to 0-1
        
        # Calculate hybrid score
        ai_weight, tfidf_weight = self._calculate_dynamic_weights(query, query_intent, ai_weight)
        hybrid_score = (tfidf_score * tfidf_weight) + (ai_score * ai_weight)
        
        result['ai_score'] = ai_score
        result['ai_reason'] = ai_result.get('reason', '')
        result['hybrid_score'] = hybrid_score
    else:
        # Fallback: Use TF-IDF score with conservative estimate
        tfidf_score = result.get('score', 0.0)
        
        # Estimate AI score based on TF-IDF (conservative)
        # If TF-IDF is high, assume AI would also score high
        estimated_ai_score = tfidf_score * 0.9  # Slightly conservative
        
        # Use lower weight for estimated scores
        estimated_ai_weight = ai_weight * 0.7  # Reduce AI influence
        estimated_tfidf_weight = 1.0 - estimated_ai_weight
        
        hybrid_score = (tfidf_score * estimated_tfidf_weight) + (estimated_ai_score * estimated_ai_weight)
        
        result['ai_score'] = estimated_ai_score
        result['ai_reason'] = 'Estimated score (AI scoring unavailable)'
        result['hybrid_score'] = hybrid_score
        result['score_estimated'] = True
        
        logger.warning(f"No AI score for result ID: {result_id}, using estimated score")
    
    reranked_results.append(result)
```

**Impact:** More robust handling of edge cases.

---

### 6. **Query-Specific Instructions** â­ HIGH PRIORITY

**Problem:** Generic instructions don't adapt to query type

**Solution:** Inject intent-based instructions into the prompt

```python
def _build_intent_specific_instructions(self, query: str, query_intent: str) -> str:
    """
    Build query-specific instructions based on detected intent.
    """
    intent_instructions = {
        'person_name': """
ðŸŽ¯ PERSON NAME QUERY DETECTED:
- Prioritize professional profiles (scs-professionals) very highly
- Articles mentioning the person should rank lower than profiles
- Exact name matches matter more than partial matches
- If person is mentioned in title, boost significantly
""",
        'service': """
ðŸŽ¯ SERVICE QUERY DETECTED:
- Prioritize service pages (scs-services) over blog posts
- Case studies about the service rank higher than general articles
- Service descriptions should rank highest
- Look for exact service name matches
""",
        'howto': """
ðŸŽ¯ HOW-TO / INFORMATIONAL QUERY DETECTED:
- Prioritize comprehensive guides and tutorials
- Step-by-step content ranks highest
- Longer, detailed content preferred over brief mentions
- Look for action words and instructions
""",
        'navigational': """
ðŸŽ¯ NAVIGATIONAL QUERY DETECTED:
- Exact page matches are critical (score 100 if exact match)
- URL structure matters (e.g., "contact" query â†’ /contact page)
- Related pages rank lower than exact matches
- Articles are usually not what user wants for navigational queries
""",
        'transactional': """
ðŸŽ¯ TRANSACTIONAL QUERY DETECTED:
- Action pages (contact, request quote) rank highest
- Service descriptions with calls-to-action rank highly
- Blog posts rank lower unless they include action steps
- Look for contact forms, buttons, and action-oriented content
"""
    }
    
    return intent_instructions.get(query_intent, "")
```

**Impact:** Better results for different query types.

---

### 7. **Score Explanation & Debugging** â­ LOW PRIORITY

**Problem:** Hard to debug why certain results rank where they do

**Solution:** Enhanced metadata with scoring breakdown

```python
# Add to metadata
metadata = {
    'ai_reranking_used': True,
    'ai_response_time': response_time,
    'ai_tokens_used': tokens_used,
    'ai_cost': cost,
    'ai_weight': ai_weight,
    'tfidf_weight': tfidf_weight,
    'custom_instructions_used': bool(custom_instructions),
    'post_type_priority_applied': bool(post_type_priority),
    'results_reranked': len(reranked_results),
    'score_distribution': {
        'min_ai_score': min([r.get('ai_score', 0) for r in reranked_results]),
        'max_ai_score': max([r.get('ai_score', 0) for r in reranked_results]),
        'avg_ai_score': sum([r.get('ai_score', 0) for r in reranked_results]) / len(reranked_results),
        'min_hybrid_score': min([r.get('hybrid_score', 0) for r in reranked_results]),
        'max_hybrid_score': max([r.get('hybrid_score', 0) for r in reranked_results]),
        'avg_hybrid_score': sum([r.get('hybrid_score', 0) for r in reranked_results]) / len(reranked_results),
    },
    'top_3_reasons': [r.get('ai_reason', '')[:100] for r in reranked_results[:3]]
}
```

**Impact:** Better debugging and monitoring.

---

### 8. **Batch Processing Optimization** â­ MEDIUM PRIORITY

**Problem:** Processing many results takes time and tokens

**Solution:** Smart batching - rerank top candidates first, then decide if more needed

```python
async def rerank_results_async_smart_batch(
    self, 
    query: str, 
    results: List[Dict[str, Any]],
    custom_instructions: str = "",
    ai_weight: float = 0.7,
    post_type_priority: Optional[List[str]] = None,
    max_results_to_rerank: int = 30  # New parameter
) -> Dict[str, Any]:
    """
    Smart batching: Only rerank top N candidates if we have many results.
    This saves tokens and time while maintaining quality.
    """
    if len(results) <= max_results_to_rerank:
        # Small result set, rerank all
        return await self.rerank_results_async(
            query, results, custom_instructions, ai_weight, post_type_priority
        )
    
    # Large result set: Rerank top candidates
    logger.info(f"Large result set ({len(results)} results), reranking top {max_results_to_rerank}")
    
    # Get top candidates (already sorted by TF-IDF)
    top_candidates = results[:max_results_to_rerank]
    
    # Rerank top candidates
    reranked = await self.rerank_results_async(
        query, top_candidates, custom_instructions, ai_weight, post_type_priority
    )
    
    # Keep remaining results with their TF-IDF scores (no AI reranking)
    remaining = results[max_results_to_rerank:]
    for result in remaining:
        result['ai_score'] = result.get('score', 0.0)  # Use TF-IDF as AI score
        result['hybrid_score'] = result.get('score', 0.0)
        result['ai_reason'] = 'Not reranked (beyond top candidates)'
    
    # Combine reranked top + remaining
    all_results = reranked['results'] + remaining
    
    return {
        'results': all_results,
        'metadata': {
            **reranked['metadata'],
            'batch_mode': True,
            'top_reranked': max_results_to_rerank,
            'total_results': len(all_results)
        }
    }
```

**Impact:** Faster reranking for large result sets.

---

## ðŸš€ Implementation Priority

### Priority 1: Immediate Impact (Implement First)
1. **Enhanced Context in Prompt** (1-2 hours)
   - Add date, categories, tags, word count to prompt
   - High impact on scoring quality

2. **Improved Scoring Criteria** (2-3 hours)
   - Better examples and guidelines
   - Context-aware scoring

### Priority 2: High Impact
3. **Query-Specific Instructions** (2-3 hours)
   - Intent-based prompt modifications
   - Better results for different query types

4. **Smart Score Normalization** (2-3 hours)
   - Ensure good score distribution
   - More consistent results

### Priority 3: Medium Impact
5. **Dynamic Hybrid Score Weighting** (3-4 hours)
   - Adaptive weights per query type
   - Better balance TF-IDF vs AI

6. **Better Error Handling** (2-3 hours)
   - Robust fallback scoring
   - Handle edge cases

### Priority 4: Performance & Monitoring
7. **Batch Processing Optimization** (2-3 hours)
   - Save tokens and time
   - Better for large result sets

8. **Score Explanation & Debugging** (1-2 hours)
   - Better monitoring
   - Easier debugging

---

## ðŸ“Š Expected Impact

### Before Improvements:
- Generic scoring
- Limited context in prompts
- Fixed weights
- Inconsistent scoring

### After Improvements:
- âœ… 25-35% better relevance for person name queries
- âœ… 20-30% better for service queries
- âœ… 30-40% better for navigational queries
- âœ… More consistent scoring across query types
- âœ… Better handling of edge cases
- âœ… Faster processing for large result sets

---

## ðŸ’¡ Quick Win: Start Here

**Implement Enhanced Context in Prompt** - This is the easiest and highest impact change:

1. Update `_format_results_for_reranking()` to include:
   - Date with freshness indicator
   - Word count
   - Categories and tags
   - Featured image status

2. Test with a few queries and compare results

3. Deploy and monitor

This alone should improve relevance by 15-20% for most queries.

---

## ðŸ“ Files to Modify

- `cerebras_llm.py`:
  - `_format_results_for_reranking()` - Enhanced formatting
  - `rerank_results_async()` - Improved scoring logic
  - `_normalize_ai_scores()` - New normalization function
  - `_calculate_dynamic_weights()` - New dynamic weighting
  - `_build_intent_specific_instructions()` - New intent instructions

Would you like me to implement any of these improvements? I recommend starting with **Enhanced Context in Prompt** and **Improved Scoring Criteria** for immediate impact.





